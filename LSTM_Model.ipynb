{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38979f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.1)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.1.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.30.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (61.2.0)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-win_amd64.whl (23.2 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.42.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.7.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.4)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 flatbuffers-23.1.21 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 keras-2.11.0 libclang-15.0.6.1 oauthlib-3.2.2 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.30.0 termcolor-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\Aniruh\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\Aniruh\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\Aniruh\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff7369a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aniruh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aniruh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "534/534 [==============================] - 209s 381ms/step - loss: 0.6500 - accuracy: 0.7947 - val_loss: 0.6423 - val_accuracy: 0.7948\n",
      "Epoch 2/20\n",
      "534/534 [==============================] - 206s 386ms/step - loss: 0.6355 - accuracy: 0.7978 - val_loss: 0.6452 - val_accuracy: 0.7943\n",
      "Epoch 3/20\n",
      "534/534 [==============================] - 212s 398ms/step - loss: 0.6283 - accuracy: 0.7991 - val_loss: 0.6426 - val_accuracy: 0.7944\n",
      "Epoch 4/20\n",
      "534/534 [==============================] - 207s 388ms/step - loss: 0.6158 - accuracy: 0.8016 - val_loss: 0.6324 - val_accuracy: 0.7940\n",
      "Epoch 5/20\n",
      "534/534 [==============================] - 209s 392ms/step - loss: 0.6089 - accuracy: 0.8051 - val_loss: 0.6471 - val_accuracy: 0.7961\n",
      "Epoch 6/20\n",
      "534/534 [==============================] - 210s 393ms/step - loss: 0.6129 - accuracy: 0.8036 - val_loss: 0.6301 - val_accuracy: 0.7948\n",
      "Epoch 7/20\n",
      "534/534 [==============================] - 219s 411ms/step - loss: 0.6020 - accuracy: 0.8066 - val_loss: 0.6487 - val_accuracy: 0.7925\n",
      "Epoch 8/20\n",
      "534/534 [==============================] - 213s 398ms/step - loss: 0.6058 - accuracy: 0.8100 - val_loss: 0.6476 - val_accuracy: 0.7958\n",
      "Epoch 9/20\n",
      "534/534 [==============================] - 210s 393ms/step - loss: 0.5270 - accuracy: 0.8176 - val_loss: 0.5264 - val_accuracy: 0.8075\n",
      "Epoch 10/20\n",
      "534/534 [==============================] - 212s 398ms/step - loss: 0.4638 - accuracy: 0.8352 - val_loss: 0.5340 - val_accuracy: 0.8147\n",
      "Epoch 11/20\n",
      "534/534 [==============================] - 213s 400ms/step - loss: 0.4308 - accuracy: 0.8468 - val_loss: 0.4872 - val_accuracy: 0.8204\n",
      "Epoch 12/20\n",
      "534/534 [==============================] - 212s 398ms/step - loss: 0.4101 - accuracy: 0.8493 - val_loss: 0.4853 - val_accuracy: 0.8196\n",
      "Epoch 13/20\n",
      "534/534 [==============================] - 5476s 10s/step - loss: 0.3758 - accuracy: 0.8619 - val_loss: 0.4746 - val_accuracy: 0.8271\n",
      "Epoch 14/20\n",
      "534/534 [==============================] - 356s 666ms/step - loss: 0.3545 - accuracy: 0.8705 - val_loss: 0.4707 - val_accuracy: 0.8227\n",
      "Epoch 15/20\n",
      "534/534 [==============================] - 400s 749ms/step - loss: 0.3355 - accuracy: 0.8774 - val_loss: 0.4660 - val_accuracy: 0.8326\n",
      "Epoch 16/20\n",
      "534/534 [==============================] - 6065s 11s/step - loss: 0.3150 - accuracy: 0.8868 - val_loss: 0.4923 - val_accuracy: 0.8257\n",
      "Epoch 17/20\n",
      "534/534 [==============================] - 346s 649ms/step - loss: 0.2977 - accuracy: 0.8956 - val_loss: 0.4844 - val_accuracy: 0.8325\n",
      "Epoch 18/20\n",
      "534/534 [==============================] - 391s 733ms/step - loss: 0.2846 - accuracy: 0.9008 - val_loss: 0.5030 - val_accuracy: 0.8291\n",
      "Epoch 19/20\n",
      "534/534 [==============================] - 333s 623ms/step - loss: 0.2675 - accuracy: 0.9088 - val_loss: 0.5023 - val_accuracy: 0.8256\n",
      "Epoch 20/20\n",
      "534/534 [==============================] - 389s 729ms/step - loss: 0.2544 - accuracy: 0.9147 - val_loss: 0.5155 - val_accuracy: 0.8199\n",
      "267/267 [==============================] - 13s 49ms/step - loss: 0.5155 - accuracy: 0.8199\n",
      "Loss: 0.5155259370803833\n",
      "Accuracy: 0.8198546767234802\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Disneyland reviews dataset\n",
    "reviews_df = pd.read_csv('DisneylandReviews.csv',encoding='ISO-8859-1')\n",
    "#\n",
    "# Remove irrelevant columns\n",
    "#reviews_df = reviews_df[['Review Text', 'Rating']]\n",
    "\n",
    "# Remove missing values\n",
    "reviews_df = reviews_df.dropna()\n",
    "\n",
    "# Remove punctuations and convert to lowercase\n",
    "def preprocess_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "reviews_df['Review_Text'] = reviews_df['Review_Text'].apply(preprocess_text)\n",
    "\n",
    "# Tokenize the reviews\n",
    "reviews_df['Tokenized'] = reviews_df['Review_Text'].apply(word_tokenize)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stop_words(tokens):\n",
    "    return [word for word in tokens if not word in stop_words]\n",
    "\n",
    "reviews_df['Tokenized'] = reviews_df['Tokenized'].apply(remove_stop_words)\n",
    "\n",
    "# Convert ratings to sentiment labels\n",
    "def convert_rating_to_label(rating):\n",
    "    if rating >= 4:\n",
    "        return 'positive'\n",
    "    elif rating <= 2:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "reviews_df['Sentiment'] = reviews_df['Rating'].apply(convert_rating_to_label)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reviews_df['Tokenized'], reviews_df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create word embeddings\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_seq_length = 200\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "\n",
    "# Build the LSTM model\n",
    "embedding_size = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_size, input_length=max_seq_length))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Convert sentiment labels to one-hot encoding\n",
    "y_train_enc = pd.get_dummies(y_train)\n",
    "y_test_enc = pd.get_dummies(y_test)\n",
    "\n",
    "# Train the model\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "model.fit(X_train_pad, y_train_enc, validation_data=(X_test_pad, y_test_enc), batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test_enc)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ead95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify new reviews\n",
    "def classify_review(review_text):\n",
    "    # Preprocess the review\n",
    "    review_text = preprocess_text(review_text)\n",
    "    tokens = word_tokenize(review_text)\n",
    "    tokens = remove_stop_words(tokens)\n",
    "    sequence = tokenizer.texts_to_sequences([tokens])[0]\n",
    "    padded = pad_sequences([sequence], maxlen=max_seq_length, padding='post', truncating='post')\n",
    "\n",
    "    # Predict the sentiment\n",
    "    sentiment_probs = model.predict(padded)[0]\n",
    "    sentiment_label = np.argmax(sentiment_probs)\n",
    "    if sentiment_label == 0:\n",
    "        return 'negative'\n",
    "    elif sentiment_label == 1:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180b4bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Predicted sentiment label: positive\n"
     ]
    }
   ],
   "source": [
    "# Define a new review\n",
    "new_review = \"I had an amazing time at Disneyland! The rides were so much fun and the atmosphere was magical.\"\n",
    "\n",
    "# Preprocess the review\n",
    "new_review = preprocess_text(new_review)\n",
    "new_review_tokens = word_tokenize(new_review)\n",
    "new_review_tokens = remove_stop_words(new_review_tokens)\n",
    "new_review_seq = tokenizer.texts_to_sequences([new_review_tokens])\n",
    "new_review_pad = pad_sequences(new_review_seq, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "\n",
    "# Make a prediction using the trained model\n",
    "prediction = model.predict(new_review_pad)\n",
    "\n",
    "# Get the predicted sentiment label\n",
    "sentiment_labels = ['negative', 'neutral', 'positive']\n",
    "predicted_label = sentiment_labels[np.argmax(prediction)]\n",
    "\n",
    "# Print the predicted label\n",
    "print(f\"Predicted sentiment label: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
